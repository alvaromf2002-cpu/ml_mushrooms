import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, RandomizedSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

# Models
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier, IsolationForest
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB

# Optional explainability
try:
    import shap
except Exception:
    shap = None

plt.rcParams['figure.figsize'] = (8,5)

# %%
# 1) Data acquisition
try:
    from ucimlrepo import fetch_ucirepo
    data_obj = fetch_ucirepo(id=848)
    X = data_obj.data.features
    y = data_obj.data.targets
    df = pd.concat([X, y], axis=1)
except Exception:
    df = pd.read_csv('secondary_data_shuffled.csv')

print('Dataset shape:', df.shape)

# %%
# Problem statement and hypothesis
"""
Goal: classify mushrooms as edible or poisonous.
Hypothesis: tree-based ensemble models perform best due to mixed feature types and non-linear interactions.
"""

# %%
# EDA
print(df['class'].value_counts(normalize=True))
numeric_cols = [c for c in df.columns if df[c].dtype != 'object' and c!='class']
cat_cols = [c for c in df.columns if c not in numeric_cols and c!='class']

# example plot
sns.countplot(x='class', data=df)
plt.title('Class distribution')
plt.show()

# %%
# Feature engineering
X = df.drop('class', axis=1)
y = df['class'].map({'e':0,'p':1})

num_tf = Pipeline([('imp', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])
cat_tf = Pipeline([('imp', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore'))])

preproc_tree = ColumnTransformer([('num', SimpleImputer(strategy='median'), numeric_cols), ('cat', OneHotEncoder(handle_unknown='ignore', sparse=False), cat_cols)])
preproc_linear = ColumnTransformer([('num', num_tf, numeric_cols), ('cat', cat_tf, cat_cols)])

# %%
# Models
models = {
    'DecisionTree': Pipeline([('preproc', preproc_tree), ('clf', DecisionTreeClassifier(random_state=42))]),
    'RandomForest': Pipeline([('preproc', preproc_tree), ('clf', RandomForestClassifier(n_estimators=200, random_state=42))]),
    'Bagging': Pipeline([('preproc', preproc_tree), ('clf', BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42))]),
    'GradientBoosting': Pipeline([('preproc', preproc_tree), ('clf', GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, random_state=42))]),
    'SVM': Pipeline([('preproc', preproc_linear), ('clf', SVC(kernel='rbf', probability=True, random_state=42))]),
    'NaiveBayes': Pipeline([('preproc', preproc_linear), ('clf', GaussianNB())])
}

# %%
# Baseline model evaluation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
results = {}
for name, pipe in models.items():
    scores = cross_val_score(pipe, X, y, cv=cv, scoring='roc_auc', n_jobs=-1)
    results[name] = scores
    print(name, 'ROC AUC mean:', scores.mean())

# %%
# Hyperparameter tuning via RandomizedSearch (RF example)
rf_pipe = models['RandomForest']
param_dist = {
    'clf__n_estimators': [100, 200, 500],
    'clf__max_depth': [None, 10, 20, 50],
    'clf__max_features': ['sqrt', 'log2', 0.3, 0.5]
}
rs = RandomizedSearchCV(rf_pipe, param_dist, n_iter=10, cv=cv, scoring='roc_auc', n_jobs=-1, random_state=42)
rs.fit(X, y)
print('Best RF params:', rs.best_params_)
print('Best AUC:', rs.best_score_)

# %%
# Bayesian Optimization (using skopt, if available)
try:
    from skopt import BayesSearchCV
    from skopt.space import Real, Integer, Categorical
    search_spaces = {
        'clf__n_estimators': Integer(100, 500),
        'clf__max_depth': Integer(5, 50),
        'clf__max_features': Categorical(['sqrt', 'log2', 0.3, 0.5]),
        'clf__min_samples_split': Integer(2, 10)
    }
    bayes_cv = BayesSearchCV(rf_pipe, search_spaces, n_iter=16, cv=cv, scoring='roc_auc', random_state=42, n_jobs=-1)
    bayes_cv.fit(X, y)
    print('Bayesian best params:', bayes_cv.best_params_)
    print('Bayesian best AUC:', bayes_cv.best_score_)
except Exception as e:
    print('Bayesian optimization skipped (install scikit-optimize). Error:', e)

# %%
# Anomaly Detection with Isolation Forest (Unsupervised phase)
iso = IsolationForest(contamination=0.01, random_state=42)
# Apply on preprocessed data
X_proc = preproc_tree.fit_transform(X)
anomaly_scores = iso.fit_predict(X_proc)
df['anomaly'] = anomaly_scores
sns.countplot(x='anomaly', hue='class', data=df)
plt.title('Potential anomalies by class')
plt.show()

print(df.groupby(['class','anomaly']).size())

# %%
# Final Evaluation
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)
best_model = rs.best_estimator_
best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)
print(classification_report(y_test, y_pred))
print('ROC AUC:', roc_auc_score(y_test, best_model.predict_proba(X_test)[:,1]))

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.show()

# %%
# Explainability
from sklearn.inspection import permutation_importance
perm = permutation_importance(best_model, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)
onehot = best_model.named_steps['preproc'].named_transformers_['cat']
feat_names = list(numeric_cols) + list(onehot.get_feature_names_out(cat_cols))
imp = pd.DataFrame({'feature': feat_names, 'importance': perm.importances_mean}).sort_values('importance', ascending=False)
print(imp.head(10))

if shap is not None:
    X_test_trans = best_model.named_steps['preproc'].transform(X_test)
    explainer = shap.Explainer(best_model.named_steps['clf'], X_test_trans)
    shap_values = explainer(X_test_trans)
    shap.summary_plot(shap_values, features=X_test_trans, feature_names=feat_names)

# %%

